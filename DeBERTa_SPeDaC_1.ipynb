{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VigneshNaveen1311/Privacy-Preserved-Mental-health-Profile.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aNI1CXfP-NF",
        "outputId": "c9e505d7-73c2-461a-8ed9-677bea93e0b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Privacy-Preserved-Mental-health-Profile'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 26 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (26/26), 7.95 MiB | 6.09 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the TSV file\n",
        "tsv_file_path = '/content/Privacy-Preserved-Mental-health-Profile/SPeDaC_Dataset/SPeDaC/SPeDaC1/trainingSENS/CURATION_USER.tsv'\n",
        "\n",
        "# Initialize lists to hold the parsed data\n",
        "metadata = []\n",
        "annotations = []\n",
        "\n",
        "# Read the file\n",
        "with open(tsv_file_path, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Parse the file\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    # Skip metadata lines\n",
        "    if line.startswith('#'):\n",
        "        metadata.append(line)\n",
        "        continue\n",
        "    # Process annotation lines\n",
        "    if line:\n",
        "        parts = line.split('\\t')\n",
        "        # Assuming the structure is consistent with the provided example\n",
        "        annotations.append({\n",
        "            'TokenId': parts[0],\n",
        "            'CharacterOffsets': parts[1],\n",
        "            'Token': parts[2],\n",
        "            'Label': parts[3]\n",
        "        })\n",
        "\n",
        "# Convert the annotations list to a DataFrame\n",
        "df_annotations = pd.DataFrame(annotations)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_annotations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1MgZYMBTVLP",
        "outputId": "291ab701-632f-4538-e8c0-d90620773f6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         TokenId CharacterOffsets      Token             Label\n",
            "0            1-1              0-8   Training                 _\n",
            "1            2-1            10-13        The  Non-sensitive[1]\n",
            "2            2-2            14-20     Tories  Non-sensitive[1]\n",
            "3            2-3            21-24        are  Non-sensitive[1]\n",
            "4            2-4            25-33   believed  Non-sensitive[1]\n",
            "...          ...              ...        ...               ...\n",
            "282930  10685-17  1386898-1386901        any  Sensitive[10682]\n",
            "282931  10685-18  1386902-1386911  political  Sensitive[10682]\n",
            "282932  10685-19  1386912-1386917      party  Sensitive[10682]\n",
            "282933  10685-20  1386918-1386919          .  Sensitive[10682]\n",
            "282934  10685-21  1386920-1386921          \"  Sensitive[10682]\n",
            "\n",
            "[282935 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Privacy-Preserved-Mental-health-Profile"
      ],
      "metadata": {
        "id": "OgENHLZk4CgH",
        "outputId": "bf7526c7-df38-4087-bac7-7e6a2724a34a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Privacy-Preserved-Mental-health-Profile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status\n"
      ],
      "metadata": {
        "id": "tR1ROet44GXc",
        "outputId": "8e3ae8be-a69d-433f-8827-53586c2eab7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"svigneshnaveen1311\"\n",
        "!git config --global user.name \"VigneshNaveen1311\""
      ],
      "metadata": {
        "id": "66f0LkNW4fAw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add DeBERTa_SPeDaC_1.ipynb\n",
        "!git commit -m \"Add DeBERTa_SPeDaC_1.ipynb\"\n"
      ],
      "metadata": {
        "id": "nTPFLRHM5IdV",
        "outputId": "745501af-6bd1-4c8d-8067-c371b7607b87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: pathspec 'DeBERTa_SPeDaC_1.ipynb' did not match any files\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"updated training\""
      ],
      "metadata": {
        "id": "YIU3DS6c4WDL",
        "outputId": "342d3a13-a9fa-4bf6-df4c-b14c1f2124f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add DeBERTa_SPeDaC_1.ipynb"
      ],
      "metadata": {
        "id": "vD6qTCFG4MXj",
        "outputId": "0f0a750b-925e-4989-f899-8321daef8c42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: pathspec 'DeBERTa_SPeDaC_1.ipynb' did not match any files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin \"https://github.com/VigneshNaveen1311/Privacy-Preserved-Mental-health-Profile\""
      ],
      "metadata": {
        "id": "Rhg2_do13S6J",
        "outputId": "816a244d-5f37-4b2a-9279-44e150fe0b36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DebertaTokenizer, DebertaForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "AXw_unLBe3Hs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d9cd036b-97ab-469a-e382-ac2b724e268b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-10289dfc6acf>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDebertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDebertaForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_output\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SetOutputMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m from .utils._tags import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_joblib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# joblib imports may raise DeprecationWarning on certain Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompressor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_compressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort_format_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdisk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmemstr_to_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n\u001b[0m\u001b[1;32m     32\u001b[0m                                  \u001b[0mThreadingBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialBackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                  LokyBackend)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPoolManagerMixin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;34m\"\"\"A helper class for managing pool of workers.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your dataset class\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, annotations, tokenizer, max_len):\n",
        "        self.annotations = annotations\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.annotations[item]['Token']\n",
        "        label = 0 if self.annotations[item]['Label'] == 'Non-sensitive' else 1\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "lfSjyVZoe7Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and parse the TSV file\n",
        "def load_dataset(file_path, tokenizer, max_len):\n",
        "    annotations = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('#'):  # Skip metadata\n",
        "            continue\n",
        "        if line:  # Process annotation lines\n",
        "            parts = line.split('\\t')\n",
        "            annotations.append({\n",
        "                'TokenId': parts[0],\n",
        "                'CharacterOffsets': parts[1],\n",
        "                'Token': parts[2],\n",
        "                'Label': parts[3]\n",
        "            })\n",
        "    return TextClassificationDataset(annotations=annotations, tokenizer=tokenizer, max_len=max_len)\n"
      ],
      "metadata": {
        "id": "X1AYG-kTe-fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')"
      ],
      "metadata": {
        "id": "u4NolHCHcVTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 256"
      ],
      "metadata": {
        "id": "kWgODYLHcmir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your training and validation datasets\n",
        "training_file_path = '/content/Privacy-Preserved-Mental-health-Profile/SPeDaC_Dataset/SPeDaC/SPeDaC1/trainingSENS/CURATION_USER.tsv'\n",
        "validation_file_path = '/content/Privacy-Preserved-Mental-health-Profile/SPeDaC_Dataset/SPeDaC/SPeDaC1/validationSENS/CURATION_USER.tsv'"
      ],
      "metadata": {
        "id": "txeem77HdGhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(training_file_path, tokenizer, max_len)\n",
        "\n",
        "val_dataset = load_dataset(validation_file_path, tokenizer, max_len)"
      ],
      "metadata": {
        "id": "ePYPmNGmdWDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "actual code"
      ],
      "metadata": {
        "id": "uAtH9rbNdLHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create DataLoader\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "Zf7KPoYDcXFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using only ten percent for checking"
      ],
      "metadata": {
        "id": "ep1oHWeIc09F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import random\n",
        "\n",
        "# Assuming 'train_dataset' is your original training dataset\n",
        "total_samples = len(train_dataset)\n",
        "subset_size = int(0.1 * total_samples)\n",
        "subset_indices = random.sample(range(total_samples), subset_size)\n",
        "random_subset = Subset(train_dataset, subset_indices)\n",
        "\n",
        "# Create DataLoader for the random subset\n",
        "train_dataloader = DataLoader(random_subset, batch_size=8, shuffle=True)\n",
        "\n",
        "# 'train_dataloader' now contains batches from the random 10% subset\n"
      ],
      "metadata": {
        "id": "QZTh7JR6c0Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create DataLoader\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
        "\n"
      ],
      "metadata": {
        "id": "1__ZR0KKfBzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the DeBERTa model\n",
        "model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=2)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "id": "V8fQrsgNcNV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        # print(\"outputs: \",outputs)\n",
        "        loss = torch.nn.functional.cross_entropy(outputs.logits, labels)\n",
        "        # print(\"LOSS: \",loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    print(\"validation started\")\n",
        "    val_accuracy = []\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "        val_accuracy.append(acc)\n",
        "\n",
        "    avg_val_accuracy = sum(val_accuracy) / len(val_accuracy)\n",
        "    print(f\"Validation accuracy: {avg_val_accuracy}\")\n"
      ],
      "metadata": {
        "id": "D1PPD-akUVnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model after training\n",
        "model_save_path = 'D:\\\\Spedac model'  # Replace with your desired save directory\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n"
      ],
      "metadata": {
        "id": "zbvP4_aXfZbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model after training\n",
        "model_save_path = 'your_model_directory'  # Replace with your desired save directory\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "VnfY4YKYeNQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b1JMSrclgDu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: connect with git repo\n",
        "\n",
        "!git init\n",
        "!git remote add origin https://github.com/VigneshNaveen1311/Privacy-Preserved-Mental-health-Profile.git\n",
        "!git pull origin main\n"
      ],
      "metadata": {
        "id": "FG8E2OlMfq22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: git commit this file\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Trained DeBERTa model and saved it to directory\"\n"
      ],
      "metadata": {
        "id": "XQ4KWF0EfaCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set the path to the TSV file within the cloned repository\n",
        "# tsv_file_path = '/content/Privacy-Preserved-Mental-health-Profile/SPeDaC_Dataset/SPeDaC/SPeDaC1/trainingSENS/CURATION_USER.tsv'\n",
        "\n",
        "# # Load the TSV file into a pandas DataFrame\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv(tsv_file_path, delimiter='\\t', header=0)\n",
        "\n",
        "# # Verify that it loaded correctly\n",
        "# print(df.head())"
      ],
      "metadata": {
        "id": "8Ix1TE-GQmi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# import pandas as pd\n",
        "\n",
        "# # URL to the raw TSV file in the GitHub repository\n",
        "# tsv_file_url = 'https://raw.githubusercontent.com/VigneshNaveen1311/Privacy-Preserved-Mental-health-Profile/main/SPeDaC_Dataset/SPeDaC/SPeDaC1/trainingSENS/CURATION_USER.tsv?token=GHSAT0AAAAAACNXK6KSSPFVFPWKJI5LWSIYZN6QMAQ'\n",
        "# # Make a request to get the content of the TSV file\n",
        "# response = requests.get(tsv_file_url)\n",
        "# response.raise_for_status()  # This will raise an exception for HTTP errors\n",
        "\n",
        "# # Read the content into a pandas DataFrame\n",
        "# df = pd.read_csv(pd.compat.StringIO(response.text), delimiter='\\t')\n"
      ],
      "metadata": {
        "id": "yNEzPmPq1yG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6AQg3MX04c9"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from transformers import DebertaTokenizer, DebertaForSequenceClassification, AdamW\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "# # Define your dataset class\n",
        "# class TextClassificationDataset(Dataset):\n",
        "#     def __init__(self, texts, labels, tokenizer, max_len):\n",
        "#         self.texts = texts\n",
        "#         self.labels = labels\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_len = max_len\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.texts)\n",
        "\n",
        "#     def __getitem__(self, item):\n",
        "#         text = str(self.texts[item])\n",
        "#         label = self.labels[item]\n",
        "#         encoding = self.tokenizer.encode_plus(\n",
        "#             text,\n",
        "#             add_special_tokens=True,\n",
        "#             max_length=self.max_len,\n",
        "#             return_token_type_ids=False,\n",
        "#             padding='max_length',\n",
        "#             truncation=True,\n",
        "#             return_attention_mask=True,\n",
        "#             return_tensors='pt',\n",
        "#         )\n",
        "#         return {\n",
        "#             'input_ids': encoding['input_ids'].flatten(),\n",
        "#             'attention_mask': encoding['attention_mask'].flatten(),\n",
        "#             'labels': torch.tensor(label, dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# # Load the dataset\n",
        "# def load_dataset(file_path, tokenizer, max_len):\n",
        "#     df = pd.read_csv(file_path, delimiter='\\t', header=0)\n",
        "#     texts = df['text_column_name'].tolist()  # Replace with your text column name\n",
        "#     labels = df['label_column_name'].tolist() # Replace with your label column name\n",
        "#     return TextClassificationDataset(texts=texts, labels=labels, tokenizer=tokenizer, max_len=max_len)\n",
        "\n",
        "# # Initialize the tokenizer\n",
        "# tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
        "\n",
        "# # Load your dataset\n",
        "# dataset_file_path = 'path_to_your_tsv_file.tsv'  # Replace with the path to your TSV file\n",
        "# max_len = 256\n",
        "# dataset = load_dataset(dataset_file_path, tokenizer, max_len)\n",
        "\n",
        "# # Create a DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# # Initialize the DeBERTa model\n",
        "# model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=2)\n",
        "\n",
        "# # Use GPU if available\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# # Optimizer and loss function\n",
        "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "# loss_fn = BCEWithLogitsLoss()\n",
        "\n",
        "# # Training loop\n",
        "# model.train()\n",
        "# for epoch in range(3):\n",
        "#     for batch in dataloader:\n",
        "#         input_ids = batch['input_ids'].to(device)\n",
        "#         attention_mask = batch['attention_mask'].to(device)\n",
        "#         labels = batch['labels'].to(device)\n",
        "\n",
        "#         model.zero_grad()\n",
        "\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#         loss = loss_fn(outputs.logits, labels.float())  # Assuming labels are 0 or 1\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "# # Save the model after training\n",
        "# model.save_pretrained('your_model_directory')  # Replace with your desired save directory\n",
        "# tokenizer.save_pretrained('your_model_directory')  # Replace with your desired save directory\n",
        "\n",
        "# # Note: You should add validation and adjust the loss function for your specific task.\n"
      ]
    }
  ]
}